{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Data Engineering: Open Data Hub and Object Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intent of this notebook is to provide examples of how data engineers/scientist can use Open Data Hub and object storage, specifically, Ceph object storage, much in the same way they are accustomed to interacting with Amazon Simple Storage Service (S3). This is made possible because Ceph's object storage gateway offers excellent fidelity with the modalities of Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Boto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boto is an integrated interface to current and future infrastructural services offered by Amazon Web Services. Among the services it provides interfaces for is Amazon S3. For lightweight analysis of data using python tools like numpy or pandas, it is handy to interact with data stored in object storage using pure python. This is where Boto shines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "\n",
    "s3_endpoint_url = os.environ['S3_ENDPOINT_URL']\n",
    "s3_access_key = os.environ['AWS_ACCESS_KEY_ID']\n",
    "s3_secret_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "s3_bucket_name = os.environ['JUPYTERHUB_USER']\n",
    "\n",
    "print(s3_endpoint_url)\n",
    "print(s3_bucket_name)\n",
    "s3 = boto3.client('s3','us-east-1', endpoint_url= s3_endpoint_url,\n",
    "                       aws_access_key_id = s3_access_key,\n",
    "                       aws_secret_access_key = s3_secret_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a bucket, uploading an object (put), and listing the bucket.\n",
    "\n",
    "In the cell below we will use our boto3 connection, `s3`, to do the following: Create an S3 bucket, upload an object, and then display all of the contents of that bucket.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3.create_bucket(Bucket=s3_bucket_name)\n",
    "s3.put_object(Bucket=s3_bucket_name,Key='object',Body='data')\n",
    "for key in s3.list_objects(Bucket=s3_bucket_name)['Contents']:\n",
    "    print(key['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #1: Manage Remote Storage\n",
    "\n",
    "Let's do something slightly more more complicated and upload a small file to our new bucket. \n",
    "\n",
    "Below we have used pandas to generate a small csv file for you. Run the below cell, and then upload it to your S3 bucket. Then Display the contents of your bucket like we did above. \n",
    "\n",
    "This resource may be helpful: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html\n",
    "\n",
    "#### Objective\n",
    "\n",
    "1) Upload a csv file to your s3 bucket using `s3.upload_file()`\n",
    "\n",
    "2) List the objects currently in your bucket using `s3.list_objects()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create and save a small pandas dataframe and save it locally as a .csv file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "x = [1,2,3,4]\n",
    "y = [4,5,6,7]\n",
    "\n",
    "df  = pd.DataFrame([x,y])\n",
    "df.to_csv('new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Upload a csv file to your s3 bucket using s3.upload_file()\n",
    "\n",
    "s3.upload_file(Filename='new_data.csv',Bucket=s3_bucket_name, Key='new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. List the objects currently in your bucket using s3.list_objects()\n",
    "\n",
    "for key in s3.list_objects(Bucket=s3_bucket_name)['Contents']:\n",
    "    print(key['Key'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now you know how to interact with and manage your data store with simple data types. But what if we needed to work with larger data sets and employ some more advanced analytics tools like Spark?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Open Data Hub operator will also install Spark. Each Jupyterhub user will also have a dedicated Spark cluster (Master and Workers) to use. The first step is to connect to the Spark Cluster and get a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "import socket\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "# Retrieve the notebook pod clusterIP to pass to the spark cluster \n",
    "spark_driver_host = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"--conf spark.jars.ivy={os.environ['HOME']} --conf spark.driver.host={spark_driver_host} --packages com.amazonaws:aws-java-sdk:1.8.0,org.apache.hadoop:hadoop-aws:2.8.5 pyspark-shell\"\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "spark = SparkSession.builder.master(spark_cluster_url).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_key)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "spark.range(100, numPartitions=100).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #2: Display Spark Data\n",
    "\n",
    "Now that we are connected to our spark cluster, let's go ahead an interact with our data. First, lets read in our initial data object we created above using `spark.read.text()` into a spark dataframe called `df0`, then we can go ahead and print the total number of rows in our dataframe using built-in `count()` method. \n",
    "\n",
    "Let's also go ahead and print out the dataframes schema, show all entire dataframe then filter it down to show just the values. \n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "1) Read in the data as a spark data frame called `df0` using `spark.read.csv()` with the parameter `header=true`\n",
    "\n",
    "2) Print the total number of rows using `df0.count()`\n",
    "\n",
    "3) Display the data frames Schema using `df0.printSchema()`\n",
    "\n",
    "4) Output the entire data frame using `df0.show()`\n",
    "\n",
    "5) Filter the output to just the values using `df0.select()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the data as a spark data frame called df0\n",
    "\n",
    "df0 = spark.read.csv(f\"s3a://{s3_bucket_name}/new_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the total number of rows using df0.count()\n",
    "\n",
    "print(\"Total number of rows in df0: %d\" % df0.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Display the data frames Schema using `df0.printSchema()'\n",
    "\n",
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Output the entire data frame using df0.show()\n",
    "\n",
    "df0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Filter the output to just the values of one column using `df0.select()\n",
    "\n",
    "df0.select(\"1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Hybrid Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of Hadoop 2.8, S3A supports per bucket configuration. This is very powerful. It allows us to have a distinct S3A configuration, with a different endpoint and different set of credentials. With this I can use a single Spark context to read a parquet file from a bucket in the public cloud (Amazon S3) into a data frame, then turn around and write that dataframe as a parquet file into a bucket that exists in the Ceph(Rook) local cluster installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf=spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.endpoint\", \"s3.amazonaws.com\")\n",
    "hadoopConf.set(\"fs.s3a.bucket.bd-dist.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise #3: Public to Private ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our hadoop configuration set up, let's do some public to private ETL.\n",
    "\n",
    "Simply read a tab separated data from a bucket in Amazon S3 and write it back out to a bucket in our Ceph(Rook) service.\n",
    "\n",
    "First, go ahead and read the tab separated file \"s3a://bd-dist/trip_report.tsv\" from our AWS bucket using Spark, then go ahead and do what we did above, printing the total number of rows, printing the schema and then displaying the entire data frame.\n",
    "\n",
    "After we've read the data into our notebook, we can go ahead an write back out to our local bucket. \n",
    "\n",
    "\n",
    "#### Objectives:\n",
    "\n",
    "1) Read in the tsv file using `spark.read.csv()` into the variable `tripreport` and make sure to pass the parameter `sep=\\t`.\n",
    "\n",
    "2) Print the total number of rows in `tripreport` using `.count()`\n",
    "\n",
    "3) Display the dataframe's schema with `.printSchema()`\n",
    "\n",
    "4) Output the dataframe with `.show()`\n",
    "\n",
    "5) Chain the `.read()` and `.write()` methods to read data from AWS and write it Ceph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read in the tsv file using `spark.read.csv()` into the variable `tripreport`.\n",
    "\n",
    "tripreport = spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Print the total number of rows in `tripreport` using `.count()`\n",
    "    \n",
    "print(\"Total number of rows in tripreport: %d\" % tripreport.count())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Display the dataframe's schema with `.printSchema()`\n",
    "\n",
    "tripreport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Output the dataframe with `.show()`\n",
    "\n",
    "tripreport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Chain the `.read()` and `.write()` methods to read data from AWS and write it Ceph\n",
    "\n",
    "spark.read.csv(\"s3a://bd-dist/trip_report.tsv\",sep=\"\\t\").write.csv(f\"s3a://{s3_bucket_name}/trip_report.tsv\",sep=\"\\t\",mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract all JSON files from a bucket prefix (pseudo directory) in S3 to `jsonFile` and use it with SparkSQL in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFile= spark.read.option(\"multiline\", True).option(\"mode\", \"PERMISSIVE\").json(\"s3a://bd-dist/kube-metrics\")\n",
    "print(\"Total number of rows in df0: %d\" % jsonFile.count())\n",
    "jsonFile.printSchema()\n",
    "jsonFile.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Import statistics libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Display schema of files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Display schema:')\n",
    "jsonFile.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Query the JSON data using filters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the created SchemaRDD as a temporary table.\n",
    "jsonFile.registerTempTable(\"kubelet_docker_operations_latency_microseconds\")\n",
    "\n",
    "#Filter the results into a data frame\n",
    "data = spark.sql(\"SELECT values, metric.operation_type FROM kubelet_docker_operations_latency_microseconds WHERE metric.quantile='0.9' AND metric.hostname='free-stg-master-03fb6'\")\n",
    "\n",
    "data.show()\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read Spark RDD into pandas dataframe for analysis. (https://spark.apache.org/docs/1.3.0/api/python/pyspark.sql.html) \n",
    "data_pd = data.toPandas()\n",
    "\n",
    "# set the operation type.\n",
    "OP_TYPE = 'list_images'\n",
    "\n",
    "# create an empty dataframe with 3 columns to populate with json data\n",
    "df2 = pd.DataFrame(columns = ['utc_timestamp','value', 'operation_type'])\n",
    "\n",
    "# for each unique operation type collect its values, and place in the new data frame\n",
    "for op in set(data_pd['operation_type']):\n",
    "    dict_raw = data_pd[data_pd['operation_type'] == op]['values']\n",
    "    list_raw = []\n",
    "    for key in dict_raw.keys():\n",
    "        list_raw.extend(dict_raw[key])\n",
    "    temp_frame = pd.DataFrame(list_raw, columns = ['utc_timestamp','value'])\n",
    "    temp_frame['operation_type'] = op\n",
    "    \n",
    "    df2 = df2.append(temp_frame)\n",
    "\n",
    "# Remove rows that contain nan values\n",
    "df2 = df2[df2['value'] != 'NaN']\n",
    "\n",
    "# convert all values to ints\n",
    "df2['value'] = df2['value'].apply(lambda a: int(a))\n",
    "\n",
    "# convert timestamp column of strings to timestamp objects\n",
    "df2['timestamp'] = df2['utc_timestamp'].apply(lambda a : datetime.fromtimestamp(int(a)))\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify Above Alerts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store timestamp with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.reset_index(inplace =True)\n",
    "\n",
    "del df2['index']\n",
    "\n",
    "df2['operation_type'].unique()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Dataframe in local Ceph(Rook)\n",
    "\n",
    "Now that we have done some work with our data we want to save it back into our local Ceph bucket. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excersie #4: Saving Your DataFrame\n",
    "\n",
    "Let's go ahead and convert our pandas dataframe back into a Spark data frame and use `.write.csv()` to write our new file.\n",
    "\n",
    "#### Objective\n",
    "\n",
    "1) Use `dfSpark.write.csv()` as `operationainfo.csv` to the `kube-metrics` location in your ceph bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas DataFrame to Spark dataframe\n",
    "dfSpark = spark.createDataFrame(df2)\n",
    "dfSpark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use `dfSpark.write.csv()` as `operationainfo.csv` to the `kube-metrics` location in your ceph bucket. \n",
    "\n",
    "dfSpark.write.csv(f\"s3a://{s3_bucket_name}//kube-metrics/operationinfo.csv\",sep=\"\\t\",mode=\"overwrite\",header = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
