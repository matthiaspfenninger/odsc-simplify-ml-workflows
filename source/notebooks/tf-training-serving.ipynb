{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and serving TensorFlow models with Kubernetes Jobs and Seldon\n",
    "\n",
    "In this notebook we will guide you through one of the most important tasks for the cloud-native data scientist: Deploying ML models as a service. This is a critical step in the intelligent application pipeline as it moves machine learning algorithms out of one-off proof-of-concepts and into a production setting where it can be easily utilized as part of a larger micro-service architecture.\n",
    "\n",
    "To do this we will use the OpenShift client tool (`oc`) to build, train and deploy a TensorFlow model on OpenShift. Most of the steps outlined in this notebook are intended to be entered directly into the command line, however, for the sake of explainability and reproducibility, we will be using the `%%bash` and `!` which allow us to execute to the command line from within a Jupyter notebook cell.  \n",
    "\n",
    "\n",
    "In this notebook we will walk through:\n",
    "\n",
    "   1) **Setting up OpenShift for Model Serving and Training**\n",
    "\n",
    "   2) **Model Training**\n",
    "\n",
    "   3) **Model Serving**\n",
    "\n",
    "   4) **Interacting with a Model Service**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup OpenShift for Model Serving and Training  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install the OC client\n",
    "First, we need to install the `oc` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "curl -O https://mirror.openshift.com/pub/openshift-v4/clients/oc/4.1/linux/oc.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp oc /opt/app-root/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Login in to your cluster and new project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to login to OpenShift server and switch projects to the one where this Jupyter server is running. We rely on two pre-configured environment variable - `$TOKEN` and `$NAMESPACE` here. There are 2 reasons for this - 1. to make the notebook reproducible without users having to manually change anything and 2. to avoid displaying the secret (`$TOKEN`) in the Jupyter UI.\n",
    "\n",
    "_If this step fails you might need to go to `Control Panel > Stop My Server` and provide those environment variables in Spawner UI_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "1) Use the `oc login` command to access your OpenShift cluster. Be sure to also pass the parameters `--server https://openshift.default.svc.cluster.local`, `--insecure-skip-tls-verify` , `--token=`$TOKEN\n",
    "\n",
    "2) Switch to your new project namespace using the `oc project [project-name]` command. Your project name should be stored in the variable ${NAMESPACE}.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** if you want to input a terminal command using Jupyter notebook cells, it can be done by adding the `%%bash` line to the top of the cell or the `!` character at the beginning of a line.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 1. Login\n",
    "oc login --server https://openshift.default.svc.cluster.local --insecure-skip-tls-verify --token=$TOKEN\n",
    "\n",
    "# 2. Switch to your project \n",
    "oc project ${NAMESPACE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply training job resources to our cluster namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we deploy our training job, we need to apply the correct resources available in the https://gitlab.com/opendatahub/data-engineering-and-machine-learning-workshop repository. These contain the necessary `BuildConfigs` and `Templates` to build and deploy the training `Job` and serving `SeldonDeployment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc apply -f ../tf-random-forest/openshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command will kick off a number of container image builds. We will need this images properly deployed in our namespace to successfully run the training jobs. \n",
    "\n",
    "While we wait, let's use the `oc logs` command to follow the build logs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc logs -f buildconfig.build.openshift.io/forest-mnist-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Examine the training job parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the parameters we can configure for the training job. Some of them come with default value, but some of them need to be configured by the user. We can output these parameters by passing the `--parameters` flag to the `oc process` command for our `forest-mnist-train` build we just did above.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "1) Output the parameters for this job using the oc command `process forest-mnist-train --parameters` in the cell below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# 1. Display the job parameters\n",
    "\n",
    "oc process forest-mnist-train --parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the predefined environment variables here again. The `MODEL_VERSION` parameter allows you to version your models - the value will be used for generation of the exported model file name so you will be able to switch between trained models in serving part.\n",
    "\n",
    "Do not forget to change `MODEL_VERSION` for each training though otherwise the following command will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc process forest-mnist-train \\\n",
    "-p S3_ENDPOINT_URL=${S3_ENDPOINT_URL} \\\n",
    "-p AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n",
    "-p AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n",
    "-p BUCKET_NAME=${NAMESPACE} \\\n",
    "-p MODEL_VERSION=\"1\" | oc apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can watch the training output by using the `oc logs` command shown in the cell below.\n",
    "\n",
    "Do not forget to change the name of job based on the output of the command above!\n",
    "\n",
    "If you are interested to see how well your model performs, you can find the `Test Accuracy` value close to the end of the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!oc logs -f job.batch/forest-mnist-train-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job outputs a compressed model into S3 object storage (using the endpoint and credentials from the environment variables). It also creates a bucket if one does not already exists.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine your trained model stored in your remote bucket\n",
    "\n",
    "Let's take a look at what buckets exists in the object storage and see the trained model stored in your bucket.\n",
    "\n",
    "If you changed the bucket name for the training job, make sure you use the same value here in `Bucket=` parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "conn = boto3.client(service_name='s3', \n",
    "                    endpoint_url=os.environ['S3_ENDPOINT_URL'])\n",
    "\n",
    "bucket = os.environ['NAMESPACE']\n",
    "pprint(conn.list_buckets()['Buckets'])\n",
    "objects = conn.list_objects(Bucket=bucket)\n",
    "\n",
    "pprint(objects)\n",
    "print(\"Stored models: \", \", \".join([x['Key'] for x in objects['Contents']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Serving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is trained, exported and stored in object object storage, we can serve it using Seldon. Let's take a look at the parameters for our deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc process forest-mnist-serve --parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see they are very similar to the training job parameters, which means we will need to provide the S3 storage credentials again and make sure `MODEL_VERSION` match so that we deploy the correct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Trained Model as a Service\n",
    "\n",
    "Use the `oc process` and `oc apply` commands to deploy our model service with the appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc process forest-mnist-serve \\\n",
    "-p S3_ENDPOINT_URL=${S3_ENDPOINT_URL} \\\n",
    "-p AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\\n",
    "-p AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\\n",
    "-p BUCKET_NAME=${NAMESPACE} \\\n",
    "-p MODEL_VERSION=\"1\" | oc apply -f -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Display pods and logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model is being deployed, use the `oc get pods` command to display the pods with the name \"forest-mnist-predictor\". After that, us the `oc logs` command again to introspect into the pod and see whats going on.\n",
    " \n",
    "**Objective**\n",
    "\n",
    "1) Use the `oc get pods` command with the `-o name` flag and filter to the predictor pod with `| grep forest-mnist-predictor`  \n",
    "\n",
    "2) Display the pod logs using `oc logs -c forest-experiment pod/[POD_NAME]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Display pods\n",
    "\n",
    "!oc get pods -o name | grep forest-mnist-predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. Inspect Logs\n",
    "\n",
    "!oc logs -c forest-experiment pod/forest-mnist-predictor-28e5946-79c4996dd8-fp9z8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interact with Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the serving container has started successfully, we can load some data into our notebook (using the TF examples library) and test out our newly deployed model inference service!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.13.*\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify our served model's route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `oc get route` command again to get the URL of the model prediction endpoint and store it as Python variable and Shell variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route=!oc get route forest-mnist -o \"jsonpath={.spec.host}\"\n",
    "route=route[0]\n",
    "%env SELDON_ROUTE=$route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select test sample\n",
    "Next we can select our test sample. Go ahead and change the value of variable `y`in the cell below to get a different image from the test dataset. \n",
    "\n",
    "You will see the actual label which should later match the  model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y=111\n",
    "x=[mnist.test.images[y].tolist()]\n",
    "print(\"Label: \", mnist.test.labels[y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to query the model for predictions. Let's take a look at two of them: Using a command line tool `curl` and a Python package `requests`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Curl**: In the cell below, we will export the variable `x` from the cell above as a shell environment variable and use it as a part of the payload to `/api/v0.1/predictions` endpoint.\n",
    "\n",
    "You will get a JSON back which contains probabilities for all the classes. The highest probability represents the predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$x\"\n",
    "\n",
    "curl -k -X POST -H 'Content-Type: application/json' \\\n",
    "    -d \"{'data': {'ndarray': $1}}\" \\\n",
    "https://${SELDON_ROUTE}/api/v0.1/predictions 2>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**requests**: It is a bit easier to work with the JSON objects in Python, so we can actually print the guessed label with it's probability. \n",
    "\n",
    "Does it match the `Label` printed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_label(predictions, names):\n",
    "    result = max(predictions)\n",
    "    return names[predictions.index(result)].split(\":\")[1], result\n",
    "    \n",
    "\n",
    "response = requests.post(\"https://%s/api/v0.1/predictions\" % route, json={'data': {'ndarray': x}}, verify=False).json()\n",
    "print(\"Predicted number is %s (%f) \" % (get_label(response['data']['ndarray'][0], response['data']['names'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations \n",
    "\n",
    "You have successful trained and served a Machine Learning model as a deployed application on OpenShift."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
